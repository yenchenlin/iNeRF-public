{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iNeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "sys.path.insert(0, os.path.join(ROOT_DIR, \"src\"))\n",
    "\n",
    "import json\n",
    "import util\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import make_model\n",
    "from render import NeRFRenderer\n",
    "import torchvision.transforms as T\n",
    "import tqdm\n",
    "import imageio\n",
    "import cv2\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "- `input`: the path of the source image for pixelNeRF.\n",
    "- `target`: the path of the target whose pose we want to estimate.\n",
    "- `output`: the directory to save rendered output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input': './input/1.png',\n",
    "    'target': './input/2.png',\n",
    "    'output': './pose_estimation'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the input data\n",
    "\n",
    "We show both the source image and the target image. Our goal is to 1) use pixelNeRF to generate a NeRF based on the source image and 2) estimate the relative pose between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"show_images\" style=\"border-spacing:0px;\"><tr><td style=\"padding:1px;\"><div style=\"display:flex; align-items:left;\">\n",
       "      <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "      <div>Source</div><div><img width=\"128\" height=\"128\" style=\"image-rendering:auto; object-fit:cover;\" src=\"data:image/png;base64,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\"/></div></div></div></td><td style=\"padding:1px;\"><div style=\"display:flex; align-items:left;\">\n",
       "      <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "      <div>Target</div><div><img width=\"128\" height=\"128\" style=\"image-rendering:auto; object-fit:cover;\" src=\"data:image/png;base64,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\"/></div></div></div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image_np = np.array(Image.open(config['input']).convert(\"RGB\"))\n",
    "target_image_np = np.array(Image.open(config['target']).convert(\"RGB\"))\n",
    "\n",
    "media.show_images({\n",
    "    'Source':  input_image_np,\n",
    "    'Target': target_image_np\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelNeRF\n",
    "\n",
    "Load the pixelNeRF. Make sure the `./checkpoints/srn_car/pixel_nerf_latest` exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT NAME: srn_car\n",
      "* Config file: conf/exp/srn.conf\n",
      "* Dataset format: srn\n",
      "* Dataset location: data\n",
      "Using torchvision resnet34 encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashantdandriyal/Desktop/Home/github/inerf/pixel-nerf/src/model/models.py:291: UserWarning: WARNING: checkpoints/srn_car/pixel_nerf_latest does not exist, not loaded!! Model will be re-initialized.\n",
      "If you are trying to load a pretrained model, STOP since it's not in the right place. If training, unless you are startin a new experiment, please remember to pass --resume.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def extra_args(parser):\n",
    "    parser.add_argument(\n",
    "        \"--input\",\n",
    "        \"-I\",\n",
    "        type=str,\n",
    "        help=\"Input image to condition on.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target\",\n",
    "        \"-T\",\n",
    "        type=str,\n",
    "        help=\"Target image to estimate the pose.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        \"-O\",\n",
    "        type=str,\n",
    "        default=os.path.join(ROOT_DIR, \"pose_estimation\"),\n",
    "        help=\"Output directory\",\n",
    "    )\n",
    "    parser.add_argument(\"--size\", type=int, default=128, help=\"Input image maxdim\")\n",
    "    parser.add_argument(\n",
    "        \"--out_size\",\n",
    "        type=str,\n",
    "        default=\"128\",\n",
    "        help=\"Output image size, either 1 or 2 number (w h)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--focal\", type=float, default=131.25, help=\"Focal length\")\n",
    "    parser.add_argument(\"--radius\", type=float, default=1.3, help=\"Camera distance\")\n",
    "    parser.add_argument(\"--z_near\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--z_far\", type=float, default=1.8)\n",
    "    parser.add_argument(\n",
    "        \"--elevation\",\n",
    "        \"-e\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"Elevation angle (negative is above)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_views\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of video frames (rotated views)\",\n",
    "    )\n",
    "    parser.add_argument(\"--fps\", type=int, default=15, help=\"FPS of video\")\n",
    "    parser.add_argument(\"--gif\", action=\"store_true\", help=\"Store gif instead of mp4\")\n",
    "    parser.add_argument(\n",
    "        \"--no_vid\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Do not store video (only image frames will be written)\",\n",
    "    )\n",
    "    parser.add_argument(\"--lrate\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--n_steps\", type=int, default=500, help=\"Number of steps for pose optimization.\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "args, conf = util.args.parse_args(\n",
    "    extra_args, default_expname=\"srn_car\", default_data_format=\"srn\", jupyter=True\n",
    ")\n",
    "args.resume = True\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "\n",
    "device = util.get_cuda(args.gpu_id[0])\n",
    "\n",
    "z_near, z_far = args.z_near, args.z_far\n",
    "focal = torch.tensor(args.focal, dtype=torch.float32, device=device)\n",
    "\n",
    "in_sz = args.size\n",
    "sz = list(map(int, args.out_size.split()))\n",
    "if len(sz) == 1:\n",
    "    H = W = sz[0]\n",
    "else:\n",
    "    assert len(sz) == 2\n",
    "    W, H = sz\n",
    "    \n",
    "net = make_model(conf[\"model\"]).to(device=device).load_weights(args)\n",
    "\n",
    "# Create the renderer.\n",
    "renderer = NeRFRenderer.from_conf(\n",
    "    conf[\"renderer\"], eval_batch_size=args.ray_batch_size\n",
    ").to(device=device)\n",
    "render_par = renderer.bind_parallel(net, args.gpu_id, simple_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iNeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: ./input/1.png\n",
      "Target image: ./input/2.png\n",
      "Input pose:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 1.3000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "Init pose:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 1.3000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], grad_fn=<SelectBackward>)\n",
      "Step 0, loss: 0.18002356588840485\n"
     ]
    }
   ],
   "source": [
    "image_to_tensor = util.get_image_to_tensor_balanced()\n",
    "\n",
    "# Encoding the input image.\n",
    "print(f\"Input image: {config['input']}\")\n",
    "input_image = Image.fromarray(input_image_np)\n",
    "input_image = T.Resize(in_sz)(input_image)\n",
    "input_image = image_to_tensor(input_image).to(device=device)\n",
    "input_pose = torch.eye(4)\n",
    "input_pose[2, -1] = args.radius\n",
    "\n",
    "print(f\"Target image: {config['target']}\")\n",
    "target_image = Image.fromarray(target_image_np)\n",
    "target_image = T.Resize(in_sz)(target_image)\n",
    "target_image_flatten = np.reshape(target_image, [-1, 3]) / 255.0\n",
    "target_image_flatten = torch.from_numpy(target_image_flatten).float().to(device=device)\n",
    "\n",
    "cam_pose = torch.clone(input_pose.detach()).unsqueeze(0)\n",
    "cam_pose.requires_grad = True\n",
    "\n",
    "print(\"Input pose:\")\n",
    "print(f\"{input_pose}\")\n",
    "print(\"Init pose:\")\n",
    "print(f\"{cam_pose[0]}\")\n",
    "\n",
    "# Create optimizer.\n",
    "optimizer = torch.optim.Adam(params=[cam_pose], lr=args.lrate)\n",
    "n_steps = 100 + 1\n",
    "\n",
    "# Loss.\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Sampling.\n",
    "n_rays = 1024\n",
    "sampling = 'center'\n",
    "\n",
    "# Pose optimization.\n",
    "predicted_poses = []\n",
    "fine_patches = []\n",
    "gt_patches = []\n",
    "\n",
    "for i_step in range(n_steps):\n",
    "    # Encode.\n",
    "    net.encode(\n",
    "        input_image.unsqueeze(0), input_pose.unsqueeze(0).to(device=device), focal,\n",
    "    )\n",
    "\n",
    "    render_rays = util.gen_rays(cam_pose, W, H, focal, z_near, z_far)\n",
    "    render_rays_flatten = render_rays.view(-1, 8)\n",
    "    assert render_rays_flatten.shape[0] == H*W\n",
    "    if sampling == 'random':\n",
    "        idxs_sampled = torch.randint(0, H*W, (n_rays,))\n",
    "    elif sampling == 'center':\n",
    "        frac = 0.5\n",
    "        mask = torch.zeros((H, W))\n",
    "        h_low = int(0.5*(1-frac)*H)\n",
    "        h_high = int(0.5*(1+frac)*H)\n",
    "        w_low = int(0.5*(1-frac)*W)\n",
    "        w_high = int(0.5*(1+frac)*W)\n",
    "        mask[h_low:h_high, w_low:w_high] = 1\n",
    "        mask = mask.reshape(H*W)\n",
    "\n",
    "        idxs_masked = torch.where(mask>0)[0]\n",
    "        idxs_sampled = idxs_masked[torch.randint(0, idxs_masked.shape[0], (n_rays,))]\n",
    "    elif sampling == 'patch':\n",
    "        frac = 0.25\n",
    "        mask = torch.zeros((H, W))\n",
    "        h_low = int(0.5*(1-frac)*H)\n",
    "        h_high = int(0.5*(1+frac)*H)\n",
    "        w_low = int(0.5*(1-frac)*W)\n",
    "        w_high = int(0.5*(1+frac)*W)\n",
    "        mask[h_low:h_high, w_low:w_high] = 1\n",
    "        mask = mask.reshape(H*W)\n",
    "\n",
    "        idxs_sampled = torch.where(mask>0)[0]\n",
    "\n",
    "    render_rays_sampled = render_rays_flatten[idxs_sampled].to(device=device)\n",
    "\n",
    "    rgb, _ = render_par(render_rays_sampled[None])\n",
    "    loss = mse_loss(rgb, target_image_flatten[idxs_sampled][None])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    if i_step % 10 == 0:        \n",
    "        predicted_poses.append(torch.clone(cam_pose[0]).detach().numpy())\n",
    "        fine_patches.append(torch.clone(rgb[0]).detach().cpu().numpy().reshape(32, 32, 3))\n",
    "        gt_patches.append(torch.clone(target_image_flatten[idxs_sampled]).detach().cpu().numpy().reshape(32, 32, 3))\n",
    "\n",
    "#         pose_pred = predicted_poses[-1].copy()\n",
    "#         pose_pred[2, -1] -= args.radius\n",
    "#         pose_pred = pose_input @ pose_pred\n",
    "#         error_R, error_t = compute_pose_error(pose_pred, pose_target)\n",
    "        print(f\"Step {i_step}, loss: {loss}\")\n",
    "        \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_image(patch):\n",
    "    image = np.zeros((128, 128, 3))\n",
    "    image[48:80, 48:80, :] = patch\n",
    "    image = (image * 255.0).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "# Rendering.\n",
    "overlay_frames = []\n",
    "n_poses = len(predicted_poses)\n",
    "render_poses = torch.from_numpy(np.array(predicted_poses))\n",
    "render_rays = util.gen_rays(render_poses, W, H, focal, z_near, z_far).to(device=device)\n",
    "with torch.no_grad():\n",
    "    print(\"Rendering\", n_poses * H * W, \"rays\")\n",
    "    all_rgb_fine = []\n",
    "    for rays in tqdm.tqdm(torch.split(render_rays.view(-1, 8), 80000, dim=0)):\n",
    "        rgb, _depth = render_par(rays[None])\n",
    "        all_rgb_fine.append(rgb[0])\n",
    "    _depth = None\n",
    "    rgb_fine = torch.cat(all_rgb_fine)\n",
    "    frames = (rgb_fine.view(n_poses, H, W, 3).cpu().numpy() * 255).astype(\n",
    "        np.uint8\n",
    "    )\n",
    "    target_image = (target_image_flatten.cpu().numpy().reshape([H, W, 3]) * 255.0).astype(np.uint8)\n",
    "    target_images = np.stack([np.array(target_image)]*n_poses, 0)\n",
    "    \n",
    "    im_name = os.path.basename(os.path.splitext(config['input'])[0])\n",
    "\n",
    "    frames_dir_name = os.path.join(config['output'], im_name + \"_frames\")\n",
    "    os.makedirs(frames_dir_name, exist_ok=True)\n",
    "\n",
    "    for i in range(n_poses):\n",
    "        if sampling == 'patch':\n",
    "            pred_patch_path = os.path.join(config['output'], f'./pred_patch_{i}.png')\n",
    "            pred_image = create_image(fine_patches[i])\n",
    "\n",
    "            gt_patch_path = os.path.join(config['output'], f'./gt_patch_{i}.png')\n",
    "            gt_image = create_image(gt_patches[i])\n",
    "            overlay_frame = (pred_image*0.5).astype(np.uint8) + (gt_image*0.5).astype(np.uint8)\n",
    "        else:\n",
    "            overlay_frame = (frames[i]*0.5).astype(np.uint8) + (target_images[i]*0.5).astype(np.uint8)\n",
    "        overlay_frames.append(overlay_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results\n",
    "\n",
    "We show the overlay of the image rendered with our predicted pose and the target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i, f in enumerate(overlay_frames):\n",
    "    step = i*10\n",
    "    data[f\"Step {step}\"] = f\n",
    "media.show_images(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixelnerf2",
   "language": "python",
   "name": "pixelnerf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
